{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from avalanche.evaluation.metrics.accuracy import Accuracy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "from timm.models import create_model\n",
    "from timm.models.layers import DropPath\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset\n",
    "\n",
    "# Import custom Dataset class from vtab folder\n",
    "from vtab.Cifar import CifarDataPytorch\n",
    "\n",
    "# Import Convpass function for model manipulation\n",
    "from convpass.convbyppass import set_Convpass\n",
    "\n",
    "# Import custom utility functions\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dl, test_dl, opt, scheduler, method, dataset, epoch = 100):\n",
    "    model.train()\n",
    "    model = model.cuda()\n",
    "    best_acc = 0\n",
    "    for ep in tqdm(range(epoch)):\n",
    "        model.train()\n",
    "        model = model.cuda()\n",
    "        for i, batch in enumerate(dl):\n",
    "            x, y = batch[0].cuda(), batch[1].cuda()\n",
    "            out = model(x)\n",
    "            loss = F.cross_entropy(out, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(ep)\n",
    "        if ep % 10 == 9:\n",
    "            acc, _, _ = test(model, test_dl)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                save(method, dataset, model, acc, ep)\n",
    "    model = model.cpu()\n",
    "    return model, best_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, dl):\n",
    "    model.eval()\n",
    "    acc = Accuracy()\n",
    "    total_time = 0\n",
    "    top5, total = 0, 0\n",
    "\n",
    "    model = model.cuda()\n",
    "    for batch in dl:  \n",
    "        x, y = batch[0].cuda(), batch[1].cuda()\n",
    "        start_time = time.time()\n",
    "        out = model(x).data\n",
    "        inference_time = time.time() - start_time\n",
    "        total_time += inference_time\n",
    "\n",
    "        _, pred = out.topk(5, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(y.view(1, -1).expand_as(pred))\n",
    "        top5 += correct[:5].reshape(-1).float().sum(0, keepdim=True)\n",
    "        total += y.size(0)\n",
    "\n",
    "        acc.update(out.argmax(dim=1).view(-1), y)\n",
    "\n",
    "    print(acc.result())\n",
    "    top5_acc = top5 / total\n",
    "    mean_inference_time = total_time / len(dl)\n",
    "\n",
    "    return acc.result(), mean_inference_time, top5_acc\n",
    "\n",
    "def count_finetuned_params(model):\n",
    "    num_finetuned_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_finetuned_params\n",
    "\n",
    "def count_total_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./models/convpass'):\n",
    "    os.makedirs('./models/convpass')\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar100'\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "method_name = 'convpass'\n",
    "epoch = 100\n",
    "class_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR100\n",
    "cifar100 = CifarDataPytorch(num_classes=100, data_dir='./data/cifar100', train_split_percent=80, batch_size=64)\n",
    "train_loader_cifar, val_loader_cifar, test_loader_cifar = cifar100.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_cifar\n",
    "val_loader = val_loader_cifar\n",
    "test_loader = test_loader_cifar\n",
    "\n",
    "print(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convpass Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('deit_tiny_distilled_patch16_224', pretrained=True, drop_path_rate=0.1)\n",
    "\n",
    "set_Convpass(model, 'convpass', dim=8, s=0.1, xavier_init=False, distilled=True)\n",
    "\n",
    "trainable = []\n",
    "model.reset_classifier(class_num)\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if 'adapter' in n or 'head' in n:\n",
    "        trainable.append(p)\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "opt = AdamW(trainable, lr=lr, weight_decay=wd)\n",
    "scheduler = CosineLRScheduler(opt, t_initial=100,\n",
    "                                  warmup_t=10, lr_min=1e-5, warmup_lr_init=1e-6)\n",
    "\n",
    "\n",
    "model, acc = train(model, train_loader, val_loader,\n",
    "                   opt, scheduler, method_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = load(method_name, dataset_name, model)\n",
    "\n",
    "num_finetuned_params = count_finetuned_params(model_tuned)\n",
    "num_total_params = count_total_params(model_tuned)\n",
    "acc, inference_mean, top5_acc = test(model, test_loader)\n",
    "\n",
    "print(f\"Number of parameters fine-tuned: {num_finetuned_params}\")\n",
    "print(f\"Total number of parameters: {num_total_params}\")\n",
    "print(f\"Share: {num_finetuned_params/num_total_params}\")\n",
    "\n",
    "print('Accuracy:', acc)\n",
    "print(f\"Mean inference time per batch: {inference_mean:.4f} seconds\")\n",
    "print(f'Top 5 Acc: {top5_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fully_tuned = create_model('deit_tiny_distilled_patch16_224', pretrained=True, \n",
    "                     drop_path_rate=0.1)\n",
    "model_fully_tuned.reset_classifier(class_num)\n",
    "\n",
    "for n, p in model_fully_tuned.named_parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "opt = AdamW(model_fully_tuned.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = CosineLRScheduler(opt, t_initial=100,\n",
    "                                  warmup_t=10, lr_min=1e-5, warmup_lr_init=1e-6)\n",
    "\n",
    "\n",
    "model_deit_trained_fulltuned, acc = train(model_fully_tuned, train_loader, val_loader,\n",
    "                   opt, scheduler, method_name, dataset_name)\n",
    "\n",
    "acc, inference_mean, top5_acc = test(model_deit_trained_fulltuned, test_loader)\n",
    "\n",
    "num_finetuned_params = count_finetuned_params(model_deit_trained_fulltuned)\n",
    "num_total_params = count_total_params(model_deit_trained_fulltuned)\n",
    "\n",
    "print(f\"Number of parameters fine-tuned: {num_finetuned_params}\")\n",
    "print(f\"Total number of parameters: {num_total_params}\")\n",
    "print(f\"Share: {num_finetuned_params/num_total_params}\")\n",
    "\n",
    "print('Accuracy:', acc)\n",
    "print(f\"Mean inference time per batch: {inference_mean:.4f} seconds\")\n",
    "print(f'Top 5 Acc: {top5_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear head-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_head_tuned = create_model('deit_tiny_distilled_patch16_224', pretrained=True, \n",
    "                     drop_path_rate=0.1)\n",
    "model_head_tuned.reset_classifier(class_num)\n",
    "\n",
    "trainable = []\n",
    "\n",
    "for n, p in model_head_tuned.named_parameters():\n",
    "    if 'head' in n:\n",
    "        trainable.append(p)\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "opt = AdamW(trainable, lr=lr, weight_decay=wd)\n",
    "scheduler = CosineLRScheduler(opt, t_initial=100,\n",
    "                                  warmup_t=10, lr_min=1e-5, warmup_lr_init=1e-6)\n",
    "\n",
    "\n",
    "model_deit_trained_headtuned, acc = train(model_head_tuned, train_loader, val_loader,\n",
    "                   opt, scheduler, method_name, dataset_name)\n",
    "\n",
    "acc, inference_mean, top5_acc = test(model_deit_trained_headtuned, test_loader)\n",
    "\n",
    "num_finetuned_params = count_finetuned_params(model_deit_trained_headtuned)\n",
    "num_total_params = count_total_params(model_deit_trained_headtuned)\n",
    "\n",
    "print(f\"Number of parameters fine-tuned: {num_finetuned_params}\")\n",
    "print(f\"Total number of parameters: {num_total_params}\")\n",
    "print(f\"Share: {num_finetuned_params/num_total_params}\")\n",
    "\n",
    "print('Accuracy:', acc)\n",
    "print(f\"Mean inference time per batch: {inference_mean:.4f} seconds\")\n",
    "print(f'Top 5 Acc: {top5_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
